{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Features Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The records are imported in 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AliHaidar\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of COVID images in train dataset is :191\n",
      "Number of Non COVID images in train dataset is :234\n",
      "training samples :425\n",
      "Number of COVID images in val dataset is :60\n",
      "Number of Non COVID images in val dataset is :58\n",
      "valing samples :118\n",
      "Number of COVID images in test dataset is :98\n",
      "Number of Non COVID images in test dataset is :105\n",
      "testing samples :203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 425/425 [00:06<00:00, 66.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(425, 224, 224, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 118/118 [00:01<00:00, 62.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118, 224, 224, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 203/203 [00:02<00:00, 76.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(203, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import cohen_kappa_score, accuracy_score\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from keras import layers\n",
    "from keras.applications import DenseNet121\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam,SGD\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import cohen_kappa_score, accuracy_score\n",
    "import scipy\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.io\n",
    "from skimage.transform import resize\n",
    "#from imgaug import augmenters as iaa\n",
    "from tqdm import tqdm\n",
    "import PIL\n",
    "from PIL import Image, ImageOps\n",
    "import cv2\n",
    "from sklearn.utils import class_weight, shuffle\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.applications.resnet50 import preprocess_input\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import f1_score, fbeta_score\n",
    "from keras.utils import Sequence\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import EarlyStopping\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "BATCH_SIZE=4\n",
    "\n",
    "IMG_SIZE=224\n",
    "historyfilename='model/history_model.json'\n",
    "modelweights='model/model_weights.h5'\n",
    "\n",
    "#The image names are found in the data-split folder\n",
    "filename_train_covid='../Data-split/COVID/trainCT_COVID.txt'\n",
    "filename_val_covid='../Data-split/COVID/valCT_COVID.txt'\n",
    "filename_test_covid='../Data-split/COVID/testCT_COVID.txt'\n",
    "\n",
    "filename_train_no_covid='../Data-split/NonCOVID/trainCT_NonCOVID.txt'\n",
    "filename_val_no_covid='../Data-split/NonCOVID/valCT_NonCOVID.txt'\n",
    "filename_test_no_covid='../Data-split/NonCOVID/testCT_NonCOVID.txt'\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "path_covid='../Images-processed/CT_COVID/CT_COVID/'\n",
    "path_non_covid='../Images-processed/CT_NonCOVID/CT_NonCOVID/'\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "\n",
    "#prepare training data\n",
    "with open(filename_train_covid) as f:\n",
    "    covidimages_train_path = f.read().splitlines()\n",
    "\n",
    "covidimages_train_path=[path_covid+s for s in covidimages_train_path]\n",
    "covidclasses=[1]*len(covidimages_train_path)\n",
    "print(\"Number of COVID images in train dataset is :\"+ str(len(covidclasses)))\n",
    "\n",
    "with open(filename_train_no_covid) as f:\n",
    "    no_covidimages_train_path = f.read().splitlines()\n",
    "non_covidimages_train_path=[path_non_covid+s for s in no_covidimages_train_path]\n",
    "non_covidclasses=[0]*len(non_covidimages_train_path)\n",
    "print(\"Number of Non COVID images in train dataset is :\"+ str(len(non_covidclasses)))\n",
    "\n",
    "trainpaths=covidimages_train_path+non_covidimages_train_path\n",
    "Y_train=covidclasses+non_covidclasses\n",
    "\n",
    "print(\"training samples :\"+ str(len(Y_train)))\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "\n",
    "#prepare val data\n",
    "with open(filename_val_covid) as f:\n",
    "    covidimages_val_path = f.read().splitlines()\n",
    "\n",
    "covidimages_val_path=[path_covid+s for s in covidimages_val_path]\n",
    "covidclasses=[1]*len(covidimages_val_path)\n",
    "print(\"Number of COVID images in val dataset is :\"+ str(len(covidclasses)))\n",
    "\n",
    "with open(filename_val_no_covid) as f:\n",
    "    no_covidimages_val_path = f.read().splitlines()\n",
    "non_covidimages_val_path=[path_non_covid+s for s in no_covidimages_val_path]\n",
    "non_covidclasses=[0]*len(non_covidimages_val_path)\n",
    "print(\"Number of Non COVID images in val dataset is :\"+ str(len(non_covidclasses)))\n",
    "\n",
    "valpaths=covidimages_val_path+non_covidimages_val_path\n",
    "Y_val=covidclasses+non_covidclasses\n",
    "\n",
    "print(\"valing samples :\"+ str(len(Y_val)))\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "\n",
    "#prepare test data\n",
    "with open(filename_test_covid) as f:\n",
    "    covidimages_test_path = f.read().splitlines()\n",
    "\n",
    "covidimages_test_path=[path_covid+s for s in covidimages_test_path]\n",
    "covidclasses=[1]*len(covidimages_test_path)\n",
    "print(\"Number of COVID images in test dataset is :\"+ str(len(covidclasses)))\n",
    "\n",
    "with open(filename_test_no_covid) as f:\n",
    "    no_covidimages_test_path = f.read().splitlines()\n",
    "non_covidimages_test_path=[path_non_covid+s for s in no_covidimages_test_path]\n",
    "non_covidclasses=[0]*len(non_covidimages_test_path)\n",
    "print(\"Number of Non COVID images in test dataset is :\"+ str(len(non_covidclasses)))\n",
    "\n",
    "testpaths=covidimages_test_path+non_covidimages_test_path\n",
    "Y_test=covidclasses+non_covidclasses\n",
    "\n",
    "print(\"testing samples :\"+ str(len(Y_test)))\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def equalize_light(image, limit=2, grid=(16,16), gray=False):\n",
    "    if (len(image.shape) == 2):\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)\n",
    "        gray = True\n",
    "    \n",
    "    clahe = cv2.createCLAHE(clipLimit=limit, tileGridSize=grid)\n",
    "    lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
    "    l, a, b = cv2.split(lab)\n",
    "\n",
    "    cl = clahe.apply(l)\n",
    "    limg = cv2.merge((cl,a,b))\n",
    "\n",
    "    image = cv2.cvtColor(limg, cv2.COLOR_LAB2BGR)\n",
    "    if gray: \n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    return np.uint8(image)\n",
    "\n",
    "def normalize(x):\n",
    "    \"\"\"\n",
    "        argument\n",
    "            - x: input image data in numpy array [32, 32, 3]\n",
    "        return\n",
    "            - normalized x \n",
    "    \"\"\"\n",
    "    min_val = np.min(x) #typically will be zero\n",
    "    max_val = np.max(x) #typically will be 255\n",
    "    x = (x-min_val) / (max_val-min_val)\n",
    "    return x\n",
    "def load_ben_color(path, sigmaX=10 ):\n",
    "    image = cv2.imread(path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n",
    "    image = equalize_light(image,3,(5,5))\n",
    "    #image=cv2.addWeighted ( image,4, cv2.GaussianBlur( image , (0,0) , sigmaX) ,-4 ,128)\n",
    "    image=normalize(image)  \n",
    "    return image\n",
    "\n",
    "\n",
    "\n",
    "N = len(trainpaths)\n",
    "X_train = np.empty((N, IMG_SIZE, IMG_SIZE, 3), dtype=np.uint8)\n",
    "\n",
    "for i, image_path in enumerate(tqdm(trainpaths)):\n",
    "    X_train[i, :, :, :] = load_ben_color(image_path,sigmaX=10)\n",
    "\n",
    "\n",
    "# In[14]:\n",
    "\n",
    "\n",
    "print(X_train.shape)\n",
    "\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "\n",
    "N_val = len(valpaths)\n",
    "X_val = np.empty((N_val, IMG_SIZE, IMG_SIZE, 3), dtype=np.uint8)\n",
    "\n",
    "for i, image_path in enumerate(tqdm(valpaths)):\n",
    "    X_val[i, :, :, :] = load_ben_color(image_path,sigmaX=10)\n",
    "print(X_val.shape)\n",
    "\n",
    "\n",
    "# In[16]:\n",
    "\n",
    "\n",
    "N_test = len(testpaths)\n",
    "X_test = np.empty((N_test, IMG_SIZE, IMG_SIZE, 3), dtype=np.uint8)\n",
    "\n",
    "for i, image_path in enumerate(tqdm(testpaths)):\n",
    "    X_test[i, :, :, :] = load_ben_color(image_path,sigmaX=10)\n",
    "print(X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Extracted Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "#with open(\"densenet121/extractedfeatures.pickle\", \"wb\") as f:\n",
    "#    pickle.dump((Train_Features,Train_Target,Val_Features,Val_Target,Test_Features,Test_Target,model,model_extracted), f)\n",
    "\n",
    "with open(\"model/MondayDesneNet169/extractedfeatures.pickle\", \"rb\") as f:\n",
    "    Train_Features,Train_Target,Val_Features,Val_Target,Test_Features,Test_Target,model,model_extracted = pickle.load(f) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Significance Test\n",
    "## Get the features into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1655</th>\n",
       "      <th>1656</th>\n",
       "      <th>1657</th>\n",
       "      <th>1658</th>\n",
       "      <th>1659</th>\n",
       "      <th>1660</th>\n",
       "      <th>1661</th>\n",
       "      <th>1662</th>\n",
       "      <th>1663</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001649</td>\n",
       "      <td>0.001086</td>\n",
       "      <td>6.450953e-09</td>\n",
       "      <td>0.001037</td>\n",
       "      <td>0.000465</td>\n",
       "      <td>0.000872</td>\n",
       "      <td>0.002501</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.091209</td>\n",
       "      <td>0.027025</td>\n",
       "      <td>2.226795</td>\n",
       "      <td>0.568400</td>\n",
       "      <td>0.587277</td>\n",
       "      <td>0.606502</td>\n",
       "      <td>0.145341</td>\n",
       "      <td>0.175026</td>\n",
       "      <td>0.418009</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000774</td>\n",
       "      <td>0.000774</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000949</td>\n",
       "      <td>0.000648</td>\n",
       "      <td>0.002735</td>\n",
       "      <td>0.001101</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.728008</td>\n",
       "      <td>0.499429</td>\n",
       "      <td>2.522061</td>\n",
       "      <td>0.579054</td>\n",
       "      <td>0.748308</td>\n",
       "      <td>1.501107</td>\n",
       "      <td>1.309332</td>\n",
       "      <td>0.310400</td>\n",
       "      <td>1.054950</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001329</td>\n",
       "      <td>0.001503</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000972</td>\n",
       "      <td>0.000863</td>\n",
       "      <td>0.001286</td>\n",
       "      <td>0.002688</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.645433</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.706715</td>\n",
       "      <td>0.445044</td>\n",
       "      <td>1.063933</td>\n",
       "      <td>0.773287</td>\n",
       "      <td>0.266251</td>\n",
       "      <td>0.187910</td>\n",
       "      <td>0.653035</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000936</td>\n",
       "      <td>0.001369</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000939</td>\n",
       "      <td>0.000661</td>\n",
       "      <td>0.002874</td>\n",
       "      <td>0.001335</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.906812</td>\n",
       "      <td>0.265181</td>\n",
       "      <td>2.089057</td>\n",
       "      <td>0.414063</td>\n",
       "      <td>1.441583</td>\n",
       "      <td>1.520676</td>\n",
       "      <td>0.808349</td>\n",
       "      <td>0.072189</td>\n",
       "      <td>1.048036</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001749</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.001059</td>\n",
       "      <td>0.000813</td>\n",
       "      <td>0.001496</td>\n",
       "      <td>0.002339</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.078483</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.420591</td>\n",
       "      <td>0.490259</td>\n",
       "      <td>2.063996</td>\n",
       "      <td>0.067864</td>\n",
       "      <td>0.031631</td>\n",
       "      <td>0.039502</td>\n",
       "      <td>0.412068</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1665 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1             2         3         4         5         6  \\\n",
       "0  0.001649  0.001086  6.450953e-09  0.001037  0.000465  0.000872  0.002501   \n",
       "1  0.000774  0.000774  0.000000e+00  0.000949  0.000648  0.002735  0.001101   \n",
       "2  0.001329  0.001503  0.000000e+00  0.000972  0.000863  0.001286  0.002688   \n",
       "3  0.000936  0.001369  0.000000e+00  0.000939  0.000661  0.002874  0.001335   \n",
       "4  0.001749  0.001000  0.000000e+00  0.001059  0.000813  0.001496  0.002339   \n",
       "\n",
       "          7         8    9  ...      1655      1656      1657      1658  \\\n",
       "0  0.000091  0.000017  0.0  ...  1.091209  0.027025  2.226795  0.568400   \n",
       "1  0.000024  0.000040  0.0  ...  0.728008  0.499429  2.522061  0.579054   \n",
       "2  0.000061  0.000011  0.0  ...  0.645433  0.000000  2.706715  0.445044   \n",
       "3  0.000039  0.000020  0.0  ...  0.906812  0.265181  2.089057  0.414063   \n",
       "4  0.000042  0.000010  0.0  ...  1.078483  0.000000  4.420591  0.490259   \n",
       "\n",
       "       1659      1660      1661      1662      1663  target  \n",
       "0  0.587277  0.606502  0.145341  0.175026  0.418009       1  \n",
       "1  0.748308  1.501107  1.309332  0.310400  1.054950       1  \n",
       "2  1.063933  0.773287  0.266251  0.187910  0.653035       1  \n",
       "3  1.441583  1.520676  0.808349  0.072189  1.048036       1  \n",
       "4  2.063996  0.067864  0.031631  0.039502  0.412068       1  \n",
       "\n",
       "[5 rows x 1665 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z=pd.DataFrame(Test_Features)\n",
    "z['target']=Test_Target\n",
    "z.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save the covid features to a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1654</th>\n",
       "      <th>1655</th>\n",
       "      <th>1656</th>\n",
       "      <th>1657</th>\n",
       "      <th>1658</th>\n",
       "      <th>1659</th>\n",
       "      <th>1660</th>\n",
       "      <th>1661</th>\n",
       "      <th>1662</th>\n",
       "      <th>1663</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>98.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>9.800000e+01</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>98.0</td>\n",
       "      <td>...</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>98.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.001248</td>\n",
       "      <td>0.001319</td>\n",
       "      <td>2.524223e-08</td>\n",
       "      <td>0.000934</td>\n",
       "      <td>0.000666</td>\n",
       "      <td>0.001607</td>\n",
       "      <td>0.001607</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.312127</td>\n",
       "      <td>1.019331</td>\n",
       "      <td>0.235724</td>\n",
       "      <td>2.339088</td>\n",
       "      <td>1.437488</td>\n",
       "      <td>0.652957</td>\n",
       "      <td>0.987084</td>\n",
       "      <td>0.752131</td>\n",
       "      <td>0.506920</td>\n",
       "      <td>1.063404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.000575</td>\n",
       "      <td>0.000484</td>\n",
       "      <td>1.444242e-07</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>0.000330</td>\n",
       "      <td>0.000547</td>\n",
       "      <td>0.000575</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.422191</td>\n",
       "      <td>0.760601</td>\n",
       "      <td>0.510076</td>\n",
       "      <td>2.269572</td>\n",
       "      <td>1.523873</td>\n",
       "      <td>0.449443</td>\n",
       "      <td>0.871454</td>\n",
       "      <td>0.887263</td>\n",
       "      <td>0.493338</td>\n",
       "      <td>0.743516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.000515</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000306</td>\n",
       "      <td>0.000237</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.103857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030984</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055024</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001097</td>\n",
       "      <td>0.055187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000796</td>\n",
       "      <td>0.000937</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000854</td>\n",
       "      <td>0.000436</td>\n",
       "      <td>0.001226</td>\n",
       "      <td>0.001178</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.330120</td>\n",
       "      <td>0.499322</td>\n",
       "      <td>0.007664</td>\n",
       "      <td>0.848599</td>\n",
       "      <td>0.265296</td>\n",
       "      <td>0.264102</td>\n",
       "      <td>0.313944</td>\n",
       "      <td>0.157012</td>\n",
       "      <td>0.161307</td>\n",
       "      <td>0.494646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.001145</td>\n",
       "      <td>0.001240</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>0.000625</td>\n",
       "      <td>0.001598</td>\n",
       "      <td>0.001621</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.715761</td>\n",
       "      <td>0.832277</td>\n",
       "      <td>0.063701</td>\n",
       "      <td>1.794734</td>\n",
       "      <td>0.638555</td>\n",
       "      <td>0.590388</td>\n",
       "      <td>0.676186</td>\n",
       "      <td>0.425348</td>\n",
       "      <td>0.327060</td>\n",
       "      <td>0.895540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.001705</td>\n",
       "      <td>0.001694</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.001033</td>\n",
       "      <td>0.000817</td>\n",
       "      <td>0.001982</td>\n",
       "      <td>0.002036</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.044714</td>\n",
       "      <td>1.250614</td>\n",
       "      <td>0.253263</td>\n",
       "      <td>3.389001</td>\n",
       "      <td>2.298876</td>\n",
       "      <td>0.876697</td>\n",
       "      <td>1.460955</td>\n",
       "      <td>0.920608</td>\n",
       "      <td>0.776803</td>\n",
       "      <td>1.537537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.002566</td>\n",
       "      <td>0.002654</td>\n",
       "      <td>1.037154e-06</td>\n",
       "      <td>0.001229</td>\n",
       "      <td>0.001891</td>\n",
       "      <td>0.002874</td>\n",
       "      <td>0.002992</td>\n",
       "      <td>0.000201</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.811655</td>\n",
       "      <td>3.901281</td>\n",
       "      <td>4.354335</td>\n",
       "      <td>16.258263</td>\n",
       "      <td>7.137276</td>\n",
       "      <td>2.063996</td>\n",
       "      <td>3.580933</td>\n",
       "      <td>4.336430</td>\n",
       "      <td>2.927922</td>\n",
       "      <td>3.106317</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 1664 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0          1             2          3          4          5     \\\n",
       "count  98.000000  98.000000  9.800000e+01  98.000000  98.000000  98.000000   \n",
       "mean    0.001248   0.001319  2.524223e-08   0.000934   0.000666   0.001607   \n",
       "std     0.000575   0.000484  1.444242e-07   0.000161   0.000330   0.000547   \n",
       "min     0.000245   0.000515  0.000000e+00   0.000223   0.000084   0.000306   \n",
       "25%     0.000796   0.000937  0.000000e+00   0.000854   0.000436   0.001226   \n",
       "50%     0.001145   0.001240  0.000000e+00   0.000969   0.000625   0.001598   \n",
       "75%     0.001705   0.001694  0.000000e+00   0.001033   0.000817   0.001982   \n",
       "max     0.002566   0.002654  1.037154e-06   0.001229   0.001891   0.002874   \n",
       "\n",
       "            6          7          8     9     ...       1654       1655  \\\n",
       "count  98.000000  98.000000  98.000000  98.0  ...  98.000000  98.000000   \n",
       "mean    0.001607   0.000066   0.000018   0.0  ...   3.312127   1.019331   \n",
       "std     0.000575   0.000046   0.000010   0.0  ...   2.422191   0.760601   \n",
       "min     0.000237   0.000002   0.000000   0.0  ...   0.000000   0.103857   \n",
       "25%     0.001178   0.000028   0.000011   0.0  ...   1.330120   0.499322   \n",
       "50%     0.001621   0.000061   0.000019   0.0  ...   2.715761   0.832277   \n",
       "75%     0.002036   0.000092   0.000025   0.0  ...   5.044714   1.250614   \n",
       "max     0.002992   0.000201   0.000046   0.0  ...   8.811655   3.901281   \n",
       "\n",
       "            1656       1657       1658       1659       1660       1661  \\\n",
       "count  98.000000  98.000000  98.000000  98.000000  98.000000  98.000000   \n",
       "mean    0.235724   2.339088   1.437488   0.652957   0.987084   0.752131   \n",
       "std     0.510076   2.269572   1.523873   0.449443   0.871454   0.887263   \n",
       "min     0.000000   0.030984   0.000000   0.055024   0.000000   0.000000   \n",
       "25%     0.007664   0.848599   0.265296   0.264102   0.313944   0.157012   \n",
       "50%     0.063701   1.794734   0.638555   0.590388   0.676186   0.425348   \n",
       "75%     0.253263   3.389001   2.298876   0.876697   1.460955   0.920608   \n",
       "max     4.354335  16.258263   7.137276   2.063996   3.580933   4.336430   \n",
       "\n",
       "            1662       1663  \n",
       "count  98.000000  98.000000  \n",
       "mean    0.506920   1.063404  \n",
       "std     0.493338   0.743516  \n",
       "min     0.001097   0.055187  \n",
       "25%     0.161307   0.494646  \n",
       "50%     0.327060   0.895540  \n",
       "75%     0.776803   1.537537  \n",
       "max     2.927922   3.106317  \n",
       "\n",
       "[8 rows x 1664 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covid_feat=z.loc[z['target']==1]\n",
    "covid_feat=covid_feat.drop(['target'],axis=1)\n",
    "covid_feat=covid_feat.reset_index(drop=True)\n",
    "covid_feat.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the non covid features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1654</th>\n",
       "      <th>1655</th>\n",
       "      <th>1656</th>\n",
       "      <th>1657</th>\n",
       "      <th>1658</th>\n",
       "      <th>1659</th>\n",
       "      <th>1660</th>\n",
       "      <th>1661</th>\n",
       "      <th>1662</th>\n",
       "      <th>1663</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>98.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>9.800000e+01</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>98.0</td>\n",
       "      <td>...</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>98.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.000850</td>\n",
       "      <td>0.001303</td>\n",
       "      <td>2.075173e-08</td>\n",
       "      <td>0.000965</td>\n",
       "      <td>0.000609</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>0.000964</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.947760</td>\n",
       "      <td>0.919676</td>\n",
       "      <td>1.033134</td>\n",
       "      <td>1.913849</td>\n",
       "      <td>1.035403</td>\n",
       "      <td>1.031385</td>\n",
       "      <td>0.795198</td>\n",
       "      <td>0.906766</td>\n",
       "      <td>0.917614</td>\n",
       "      <td>1.240114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.000397</td>\n",
       "      <td>0.000308</td>\n",
       "      <td>1.285661e-07</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000407</td>\n",
       "      <td>0.000534</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.251250</td>\n",
       "      <td>0.493257</td>\n",
       "      <td>0.851603</td>\n",
       "      <td>0.848680</td>\n",
       "      <td>1.013819</td>\n",
       "      <td>0.419771</td>\n",
       "      <td>0.509863</td>\n",
       "      <td>0.628759</td>\n",
       "      <td>0.701072</td>\n",
       "      <td>0.773064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000330</td>\n",
       "      <td>0.000748</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000651</td>\n",
       "      <td>0.000244</td>\n",
       "      <td>0.000741</td>\n",
       "      <td>0.000208</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.056769</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081618</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.061087</td>\n",
       "      <td>0.070033</td>\n",
       "      <td>0.076234</td>\n",
       "      <td>0.004901</td>\n",
       "      <td>0.141477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000567</td>\n",
       "      <td>0.001113</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000923</td>\n",
       "      <td>0.000495</td>\n",
       "      <td>0.001824</td>\n",
       "      <td>0.000532</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.153479</td>\n",
       "      <td>0.522047</td>\n",
       "      <td>0.438912</td>\n",
       "      <td>1.451328</td>\n",
       "      <td>0.339936</td>\n",
       "      <td>0.742516</td>\n",
       "      <td>0.428039</td>\n",
       "      <td>0.411257</td>\n",
       "      <td>0.408978</td>\n",
       "      <td>0.665186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000791</td>\n",
       "      <td>0.001258</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000982</td>\n",
       "      <td>0.000620</td>\n",
       "      <td>0.002169</td>\n",
       "      <td>0.000838</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.442188</td>\n",
       "      <td>0.888801</td>\n",
       "      <td>0.780638</td>\n",
       "      <td>1.822992</td>\n",
       "      <td>0.729092</td>\n",
       "      <td>1.087426</td>\n",
       "      <td>0.667905</td>\n",
       "      <td>0.787810</td>\n",
       "      <td>0.691819</td>\n",
       "      <td>1.029578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000937</td>\n",
       "      <td>0.001431</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.001034</td>\n",
       "      <td>0.000701</td>\n",
       "      <td>0.002355</td>\n",
       "      <td>0.001286</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.140306</td>\n",
       "      <td>1.166816</td>\n",
       "      <td>1.395865</td>\n",
       "      <td>2.157593</td>\n",
       "      <td>1.395308</td>\n",
       "      <td>1.235317</td>\n",
       "      <td>1.011737</td>\n",
       "      <td>1.193980</td>\n",
       "      <td>1.346892</td>\n",
       "      <td>1.663054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.002869</td>\n",
       "      <td>0.002289</td>\n",
       "      <td>1.109712e-06</td>\n",
       "      <td>0.001151</td>\n",
       "      <td>0.001242</td>\n",
       "      <td>0.003092</td>\n",
       "      <td>0.002319</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.081854</td>\n",
       "      <td>2.718112</td>\n",
       "      <td>3.791058</td>\n",
       "      <td>4.999496</td>\n",
       "      <td>4.754226</td>\n",
       "      <td>2.390061</td>\n",
       "      <td>2.560513</td>\n",
       "      <td>3.218781</td>\n",
       "      <td>3.128746</td>\n",
       "      <td>3.667073</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 1664 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0          1             2          3          4          5     \\\n",
       "count  98.000000  98.000000  9.800000e+01  98.000000  98.000000  98.000000   \n",
       "mean    0.000850   0.001303  2.075173e-08   0.000965   0.000609   0.002100   \n",
       "std     0.000397   0.000308  1.285661e-07   0.000102   0.000164   0.000407   \n",
       "min     0.000330   0.000748  0.000000e+00   0.000651   0.000244   0.000741   \n",
       "25%     0.000567   0.001113  0.000000e+00   0.000923   0.000495   0.001824   \n",
       "50%     0.000791   0.001258  0.000000e+00   0.000982   0.000620   0.002169   \n",
       "75%     0.000937   0.001431  0.000000e+00   0.001034   0.000701   0.002355   \n",
       "max     0.002869   0.002289  1.109712e-06   0.001151   0.001242   0.003092   \n",
       "\n",
       "            6          7          8     9     ...       1654       1655  \\\n",
       "count  98.000000  98.000000  98.000000  98.0  ...  98.000000  98.000000   \n",
       "mean    0.000964   0.000056   0.000030   0.0  ...   0.947760   0.919676   \n",
       "std     0.000534   0.000026   0.000007   0.0  ...   1.251250   0.493257   \n",
       "min     0.000208   0.000018   0.000007   0.0  ...   0.000000   0.056769   \n",
       "25%     0.000532   0.000039   0.000026   0.0  ...   0.153479   0.522047   \n",
       "50%     0.000838   0.000051   0.000030   0.0  ...   0.442188   0.888801   \n",
       "75%     0.001286   0.000067   0.000033   0.0  ...   1.140306   1.166816   \n",
       "max     0.002319   0.000153   0.000048   0.0  ...   6.081854   2.718112   \n",
       "\n",
       "            1656       1657       1658       1659       1660       1661  \\\n",
       "count  98.000000  98.000000  98.000000  98.000000  98.000000  98.000000   \n",
       "mean    1.033134   1.913849   1.035403   1.031385   0.795198   0.906766   \n",
       "std     0.851603   0.848680   1.013819   0.419771   0.509863   0.628759   \n",
       "min     0.000000   0.081618   0.000000   0.061087   0.070033   0.076234   \n",
       "25%     0.438912   1.451328   0.339936   0.742516   0.428039   0.411257   \n",
       "50%     0.780638   1.822992   0.729092   1.087426   0.667905   0.787810   \n",
       "75%     1.395865   2.157593   1.395308   1.235317   1.011737   1.193980   \n",
       "max     3.791058   4.999496   4.754226   2.390061   2.560513   3.218781   \n",
       "\n",
       "            1662       1663  \n",
       "count  98.000000  98.000000  \n",
       "mean    0.917614   1.240114  \n",
       "std     0.701072   0.773064  \n",
       "min     0.004901   0.141477  \n",
       "25%     0.408978   0.665186  \n",
       "50%     0.691819   1.029578  \n",
       "75%     1.346892   1.663054  \n",
       "max     3.128746   3.667073  \n",
       "\n",
       "[8 rows x 1664 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_covid_feat=z.loc[z['target']==0]\n",
    "non_covid_feat=non_covid_feat.drop(['target'],axis=1)\n",
    "non_covid_feat=non_covid_feat.reset_index(drop=True)\n",
    "non_covid_feat=non_covid_feat.head(98)#selected the first 98 to make sure that the number of samples are equal\n",
    "non_covid_feat.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(98, 1664)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_covid_feat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply t-test for each set of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import f_oneway,ttest_ind,ttest_rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably different distributions\n",
      "Probably the same distribution\n",
      "Probably the same distribution\n",
      "Probably different distributions\n",
      "Probably the same distribution\n"
     ]
    }
   ],
   "source": [
    "count_significance = 0\n",
    "col_sig=[]\n",
    "count_no_significance= 0\n",
    "for col in columns:\n",
    "    data1=covid_feat[col].tolist()\n",
    "    data2=non_covid_feat[col].tolist()\n",
    "    stat, p = ttest_ind(data1, data2)\n",
    "    if p > 0.05:\n",
    "        print('Probably the same distribution')\n",
    "        count_no_significance=count_no_significance+1\n",
    "    else:\n",
    "        print('Probably different distributions')\n",
    "        col_sig.append(col)\n",
    "        count_significance=count_significance+1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1203\n"
     ]
    }
   ],
   "source": [
    "print(count_significance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7229567307692307"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1203/1664 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse important features in RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RF important features\n",
    "importances = rf.feature_importances_\n",
    "columns=covid_feat.columns.tolist()\n",
    "di={'column': columns, 'importance':importances}\n",
    "featimp=pd.DataFrame(di)\n",
    "featimp = featimp.sort_values(by ='importance',ascending=False)# sort the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>984</td>\n",
       "      <td>0.014081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>120</td>\n",
       "      <td>0.010435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>557</td>\n",
       "      <td>0.010210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1563</th>\n",
       "      <td>1563</td>\n",
       "      <td>0.009571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      column  importance\n",
       "984      984    0.014081\n",
       "120      120    0.010435\n",
       "557      557    0.010210\n",
       "1563    1563    0.009571"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featimp.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### top 50 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=featimp['column'].tolist()[:50]#get the top 30 features\n",
    "#check the ones that are not in significant columns\n",
    "for i in d: \n",
    "    if i not in col_sig:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### top 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1288\n",
      "1472\n",
      "1128\n",
      "1008\n"
     ]
    }
   ],
   "source": [
    "d=featimp['column'].tolist()[:100]#get the top 30 features\n",
    "#check the ones that are not in significant columns\n",
    "for i in d: \n",
    "    if i not in col_sig:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1288\n",
      "1472\n",
      "1128\n",
      "1008\n",
      "184\n",
      "1158\n",
      "653\n",
      "1414\n",
      "3\n",
      "1395\n",
      "341\n",
      "1294\n",
      "1180\n",
      "1195\n",
      "1023\n",
      "1121\n",
      "719\n",
      "987\n",
      "1392\n",
      "1551\n",
      "80\n"
     ]
    }
   ],
   "source": [
    "d=featimp['column'].tolist()[:200]#get the top 30 features\n",
    "#check the ones that are not in significant columns\n",
    "for i in d: \n",
    "    if i not in col_sig:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.105"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "21/200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the Analysis of Variance Test\n",
    "from scipy.stats import f_oneway\n",
    "data1 = [0.873, 2.817, 0.121, -0.945, -0.055, -1.436, 0.360, -1.478, -1.637, -1.869]\n",
    "data2 = [1.142, -0.432, -0.938, -0.729, -0.846, -0.157, 0.500, 1.183, -1.075, -0.169]\n",
    "data3 = [-0.208, 0.696, 0.928, -1.148, -0.213, 0.229, 0.137, 0.269, -0.870, -1.204]\n",
    "stat, p = f_oneway(data1, data2, data3)\n",
    "print('stat=%.3f, p=%.3f' % (stat, p))\n",
    "if p > 0.05:\n",
    "\tprint('Probably the same distribution')\n",
    "else:\n",
    "\tprint('Probably different distributions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "def measure_per(y_val,preds,num_classes):\n",
    "    print(accuracy_score(y_val,preds))\n",
    "    print(cohen_kappa_score(y_val,preds))\n",
    "\n",
    "    \n",
    "    df_cm = pd.DataFrame(cm, range(num_classes),\n",
    "                              range(num_classes))\n",
    "    print(df_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes=2\n",
    "# Creating a function to report confusion metrics\n",
    "def confusion_metrics (truvalues,predictions):\n",
    "# save confusion matrix and slice into four pieces\n",
    "    conf_matrix = confusion_matrix(truvalues, predictions)\n",
    "    df_cm = pd.DataFrame(conf_matrix, range(num_classes),\n",
    "                              range(num_classes))\n",
    "    print(df_cm)\n",
    "    TP = conf_matrix[1][1]\n",
    "    TN = conf_matrix[0][0]\n",
    "    FP = conf_matrix[0][1]\n",
    "    FN = conf_matrix[1][0]\n",
    "    print('True Positives:', TP)\n",
    "    print('True Negatives:', TN)\n",
    "    print('False Positives:', FP)\n",
    "    print('False Negatives:', FN)\n",
    "    \n",
    "    # calculate accuracy\n",
    "    conf_accuracy = (float (TP+TN) / float(TP + TN + FP + FN))\n",
    "    \n",
    "    # calculate mis-classification\n",
    "    conf_misclassification = 1- conf_accuracy\n",
    "    \n",
    "    # calculate the sensitivity\n",
    "    conf_sensitivity = (TP / float(TP + FN))\n",
    "    # calculate the specificity\n",
    "    conf_specificity = (TN / float(TN + FP))\n",
    "    \n",
    "    # calculate precision\n",
    "    conf_precision = (TN / float(TN + FP))\n",
    "    # calculate f_1 score\n",
    "    conf_f1 = 2 * ((conf_precision * conf_sensitivity) / (conf_precision + conf_sensitivity))\n",
    "    print('-'*50)\n",
    "    print(f'Accuracy: {round(conf_accuracy,2)}') \n",
    "    print(f'Mis-Classification: {round(conf_misclassification,2)}') \n",
    "    print(f'Sensitivity: {round(conf_sensitivity,2)}') \n",
    "    print(f'Specificity: {round(conf_specificity,2)}') \n",
    "    print(f'Precision: {round(conf_precision,2)}')\n",
    "    print(f'f_1 Score: {round(conf_f1,2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relaod Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reload xgboost\n",
    "with open(\"Reportedmodels/bestXGBoostmodel.pickle\", \"rb\") as f:\n",
    "    xgb = pickle.load(f) \n",
    "\n",
    "with open(\"Reportedmodels/bestadaboostmodel.pickle\", \"rb\") as f:\n",
    "    adaboost = pickle.load(f) \n",
    "\n",
    "with open(\"Reportedmodels/bestbdtmodel.pickle\", \"rb\") as f:\n",
    "    bdt = pickle.load(f) \n",
    "    \n",
    "with open(\"Reportedmodels/bestdartmmodel.pickle\", \"rb\") as f:\n",
    "    dart = pickle.load(f) \n",
    "\n",
    "with open(\"Reportedmodels/bestlightgbmmodel.pickle\", \"rb\") as f:\n",
    "    lgb = pickle.load(f) \n",
    "\n",
    "with open(\"Reportedmodels/bestRFmodel.pickle\", \"rb\") as f:\n",
    "    rf = pickle.load(f) \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check acc again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9033041788143829\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier,XGBRegressor\n",
    "\n",
    "preds_xgb = xgb.predict(Test_Features)\n",
    "\n",
    "from sklearn.metrics import accuracy_score,f1_score,roc_curve,auc\n",
    "fpr, tpr, thresholds = roc_curve(Y_test, xgb.predict_proba(Test_Features)[:,1])\n",
    "print(\"AUC: \" + str(auc(fpr, tpr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0   1\n",
      "0  91  14\n",
      "1  13  85\n",
      "True Positives: 85\n",
      "True Negatives: 91\n",
      "False Positives: 14\n",
      "False Negatives: 13\n",
      "--------------------------------------------------\n",
      "Accuracy: 0.87\n",
      "Mis-Classification: 0.13\n",
      "Sensitivity: 0.87\n",
      "Specificity: 0.87\n",
      "Precision: 0.87\n",
      "f_1 Score: 0.87\n"
     ]
    }
   ],
   "source": [
    "confusion_metrics(Y_test,preds_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9011175898931001\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "preds_rf = rf.predict(Test_Features)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(Y_test, rf.predict_proba(Test_Features)[:,1])\n",
    "print(\"AUC: \" + str(auc(fpr, tpr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0   1\n",
      "0  89  16\n",
      "1  10  88\n",
      "True Positives: 88\n",
      "True Negatives: 89\n",
      "False Positives: 16\n",
      "False Negatives: 10\n",
      "--------------------------------------------------\n",
      "Accuracy: 0.87\n",
      "Mis-Classification: 0.13\n",
      "Sensitivity: 0.9\n",
      "Specificity: 0.85\n",
      "Precision: 0.85\n",
      "f_1 Score: 0.87\n"
     ]
    }
   ],
   "source": [
    "confusion_metrics(Y_test,preds_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9015549076773567\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "preds_bdt = bdt.predict(Test_Features)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(Y_test, bdt.predict_proba(Test_Features)[:,1])\n",
    "print(\"AUC: \" + str(auc(fpr, tpr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0   1\n",
      "0  86  19\n",
      "1  11  87\n",
      "True Positives: 87\n",
      "True Negatives: 86\n",
      "False Positives: 19\n",
      "False Negatives: 11\n",
      "--------------------------------------------------\n",
      "Accuracy: 0.85\n",
      "Mis-Classification: 0.15\n",
      "Sensitivity: 0.89\n",
      "Specificity: 0.82\n",
      "Precision: 0.82\n",
      "f_1 Score: 0.85\n"
     ]
    }
   ],
   "source": [
    "confusion_metrics(Y_test,preds_bdt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.8350826044703596\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "\n",
    "preds_adaboost = adaboost.predict(Test_Features)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(Y_test, adaboost.predict_proba(Test_Features)[:,1])\n",
    "print(\"AUC: \" + str(auc(fpr, tpr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0   1\n",
      "0  84  21\n",
      "1  36  62\n",
      "True Positives: 62\n",
      "True Negatives: 84\n",
      "False Positives: 21\n",
      "False Negatives: 36\n",
      "--------------------------------------------------\n",
      "Accuracy: 0.72\n",
      "Mis-Classification: 0.28\n",
      "Sensitivity: 0.63\n",
      "Specificity: 0.8\n",
      "Precision: 0.8\n",
      "f_1 Score: 0.71\n"
     ]
    }
   ],
   "source": [
    "confusion_metrics(Y_test,preds_adaboost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GBDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9023323615160349\n"
     ]
    }
   ],
   "source": [
    "import lightgbm\n",
    "\n",
    "preds_gbdt=lgb.predict(Test_Features)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(Y_test, preds_gbdt)\n",
    "print(\"AUC: \" + str(auc(fpr, tpr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0   1\n",
      "0  90  15\n",
      "1  18  80\n",
      "True Positives: 80\n",
      "True Negatives: 90\n",
      "False Positives: 15\n",
      "False Negatives: 18\n",
      "--------------------------------------------------\n",
      "Accuracy: 0.84\n",
      "Mis-Classification: 0.16\n",
      "Sensitivity: 0.82\n",
      "Specificity: 0.86\n",
      "Precision: 0.86\n",
      "f_1 Score: 0.84\n"
     ]
    }
   ],
   "source": [
    "confusion_metrics(Y_test,np.round(preds_gbdt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.897181729834791\n"
     ]
    }
   ],
   "source": [
    "import lightgbm\n",
    "\n",
    "preds_dart=dart.predict(Test_Features)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(Y_test, preds_dart)\n",
    "print(\"AUC: \" + str(auc(fpr, tpr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0   1\n",
      "0  93  12\n",
      "1  15  83\n",
      "True Positives: 83\n",
      "True Negatives: 93\n",
      "False Positives: 12\n",
      "False Negatives: 15\n",
      "--------------------------------------------------\n",
      "Accuracy: 0.87\n",
      "Mis-Classification: 0.13\n",
      "Sensitivity: 0.85\n",
      "Specificity: 0.89\n",
      "Precision: 0.89\n",
      "f_1 Score: 0.87\n"
     ]
    }
   ],
   "source": [
    "confusion_metrics(Y_test,np.round(preds_dart))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 203/203 [00:00<00:00, 183461.26it/s]\n"
     ]
    }
   ],
   "source": [
    "paths=[]\n",
    "for i, image_path in enumerate(tqdm(testpaths)):\n",
    "    paths.append(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "di={'paths':paths,'actual':Y_test,'DenseNet':np.round(np.squeeze(preds)),'preds_xgboost':preds_xgb,'preds_rf':preds_rf,\n",
    "   'preds_bdt':preds_bdt,'preds_adaboost':preds_adaboost,'preds_gbdt':np.round(preds_gbdt),'preds_dart':np.round(preds_dart)}\n",
    "\n",
    "df=pd.DataFrame(di)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paths</th>\n",
       "      <th>actual</th>\n",
       "      <th>DenseNet</th>\n",
       "      <th>preds_xgboost</th>\n",
       "      <th>preds_rf</th>\n",
       "      <th>preds_bdt</th>\n",
       "      <th>preds_adaboost</th>\n",
       "      <th>preds_gbdt</th>\n",
       "      <th>preds_dart</th>\n",
       "      <th>thes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../Images-processed/CT_COVID/CT_COVID/2020.03....</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../Images-processed/CT_COVID/CT_COVID/2020.03....</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../Images-processed/CT_COVID/CT_COVID/2020.03....</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../Images-processed/CT_COVID/CT_COVID/2020.03....</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../Images-processed/CT_COVID/CT_COVID/2020.03....</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               paths  actual  DenseNet  \\\n",
       "0  ../Images-processed/CT_COVID/CT_COVID/2020.03....       1       1.0   \n",
       "1  ../Images-processed/CT_COVID/CT_COVID/2020.03....       1       0.0   \n",
       "2  ../Images-processed/CT_COVID/CT_COVID/2020.03....       1       1.0   \n",
       "3  ../Images-processed/CT_COVID/CT_COVID/2020.03....       1       1.0   \n",
       "4  ../Images-processed/CT_COVID/CT_COVID/2020.03....       1       1.0   \n",
       "\n",
       "   preds_xgboost  preds_rf  preds_bdt  preds_adaboost  preds_gbdt  preds_dart  \\\n",
       "0              1         1          1               1         1.0         1.0   \n",
       "1              1         1          1               0         1.0         0.0   \n",
       "2              1         1          1               1         1.0         1.0   \n",
       "3              1         1          1               1         1.0         1.0   \n",
       "4              1         1          1               1         1.0         1.0   \n",
       "\n",
       "   thes  \n",
       "0   7.0  \n",
       "1   4.0  \n",
       "2   7.0  \n",
       "3   7.0  \n",
       "4   7.0  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['thes'] = df['DenseNet'] + df['preds_xgboost'] + df['preds_rf']+df['preds_bdt'] + df['preds_adaboost'] + df['preds_gbdt']+ df['preds_dart']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Results.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6 images were misclassified by all the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05714285714285714"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "6/105"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import KMeans\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create kmeans object\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "# fit kmeans object to data\n",
    "kmeans.fit(Train_Features)\n",
    "# print location of clusters learned by kmeans object\n",
    "print(kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_km = kmeans.fit_predict(Train_Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = kmeans.fit_predict(Test_Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(preds_dart)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "598.4px",
    "left": "1166px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
