{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Features Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeldir='Friday26June_DenseNet169_0.911_Adam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "historyfilename='model/'+modeldir+'/history_model_norm.json'\n",
    "modelweights='model/'+modeldir+'/model_weights_norm.h5'\n",
    "BATCH_SIZE=8\n",
    "thevars='model/'+modeldir+'/extractedfeatures_norm.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score, accuracy_score\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from keras import layers\n",
    "from keras.applications import DenseNet121\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam,SGD\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import cohen_kappa_score, accuracy_score\n",
    "import scipy\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.io\n",
    "from skimage.transform import resize\n",
    "#from imgaug import augmenters as iaa\n",
    "from tqdm import tqdm\n",
    "import PIL\n",
    "from PIL import Image, ImageOps\n",
    "import cv2\n",
    "from sklearn.utils import class_weight, shuffle\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.applications.resnet50 import preprocess_input\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import f1_score, fbeta_score\n",
    "from keras.utils import Sequence\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import EarlyStopping\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "IMG_SIZE=224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#The image names are found in the data-split folder\n",
    "filename_train_covid='../Data-split/COVID/trainCT_COVID.txt'\n",
    "filename_val_covid='../Data-split/COVID/valCT_COVID.txt'\n",
    "filename_test_covid='../Data-split/COVID/testCT_COVID.txt'\n",
    "\n",
    "filename_train_no_covid='../Data-split/NonCOVID/trainCT_NonCOVID.txt'\n",
    "filename_val_no_covid='../Data-split/NonCOVID/valCT_NonCOVID.txt'\n",
    "filename_test_no_covid='../Data-split/NonCOVID/testCT_NonCOVID.txt'\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "path_covid='../Images-processed/CT_COVID/CT_COVID/'\n",
    "path_non_covid='../Images-processed/CT_NonCOVID/CT_NonCOVID/'\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "\n",
    "#prepare training data\n",
    "with open(filename_train_covid) as f:\n",
    "    covidimages_train_path = f.read().splitlines()\n",
    "\n",
    "covidimages_train_path=[path_covid+s for s in covidimages_train_path]\n",
    "covidclasses=[1]*len(covidimages_train_path)\n",
    "print(\"Number of COVID images in train dataset is :\"+ str(len(covidclasses)))\n",
    "\n",
    "with open(filename_train_no_covid) as f:\n",
    "    no_covidimages_train_path = f.read().splitlines()\n",
    "non_covidimages_train_path=[path_non_covid+s for s in no_covidimages_train_path]\n",
    "non_covidclasses=[0]*len(non_covidimages_train_path)\n",
    "print(\"Number of Non COVID images in train dataset is :\"+ str(len(non_covidclasses)))\n",
    "\n",
    "trainpaths=covidimages_train_path+non_covidimages_train_path\n",
    "Y_train=covidclasses+non_covidclasses\n",
    "\n",
    "print(\"training samples :\"+ str(len(Y_train)))\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "\n",
    "#prepare val data\n",
    "with open(filename_val_covid) as f:\n",
    "    covidimages_val_path = f.read().splitlines()\n",
    "\n",
    "covidimages_val_path=[path_covid+s for s in covidimages_val_path]\n",
    "covidclasses=[1]*len(covidimages_val_path)\n",
    "print(\"Number of COVID images in val dataset is :\"+ str(len(covidclasses)))\n",
    "\n",
    "with open(filename_val_no_covid) as f:\n",
    "    no_covidimages_val_path = f.read().splitlines()\n",
    "non_covidimages_val_path=[path_non_covid+s for s in no_covidimages_val_path]\n",
    "non_covidclasses=[0]*len(non_covidimages_val_path)\n",
    "print(\"Number of Non COVID images in val dataset is :\"+ str(len(non_covidclasses)))\n",
    "\n",
    "valpaths=covidimages_val_path+non_covidimages_val_path\n",
    "Y_val=covidclasses+non_covidclasses\n",
    "\n",
    "print(\"valing samples :\"+ str(len(Y_val)))\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "\n",
    "#prepare test data\n",
    "with open(filename_test_covid) as f:\n",
    "    covidimages_test_path = f.read().splitlines()\n",
    "\n",
    "covidimages_test_path=[path_covid+s for s in covidimages_test_path]\n",
    "covidclasses=[1]*len(covidimages_test_path)\n",
    "print(\"Number of COVID images in test dataset is :\"+ str(len(covidclasses)))\n",
    "\n",
    "with open(filename_test_no_covid) as f:\n",
    "    no_covidimages_test_path = f.read().splitlines()\n",
    "non_covidimages_test_path=[path_non_covid+s for s in no_covidimages_test_path]\n",
    "non_covidclasses=[0]*len(non_covidimages_test_path)\n",
    "print(\"Number of Non COVID images in test dataset is :\"+ str(len(non_covidclasses)))\n",
    "\n",
    "testpaths=covidimages_test_path+non_covidimages_test_path\n",
    "Y_test=covidclasses+non_covidclasses\n",
    "\n",
    "print(\"testing samples :\"+ str(len(Y_test)))\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def equalize_light(image, limit=2, grid=(16,16), gray=False):\n",
    "    if (len(image.shape) == 2):\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)\n",
    "        gray = True\n",
    "    \n",
    "    clahe = cv2.createCLAHE(clipLimit=limit, tileGridSize=grid)\n",
    "    lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
    "    l, a, b = cv2.split(lab)\n",
    "\n",
    "    cl = clahe.apply(l)\n",
    "    limg = cv2.merge((cl,a,b))\n",
    "\n",
    "    image = cv2.cvtColor(limg, cv2.COLOR_LAB2BGR)\n",
    "    if gray: \n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    return np.uint8(image)\n",
    "\n",
    "def normalize(x):\n",
    "    \"\"\"\n",
    "        argument\n",
    "            - x: input image data in numpy array [32, 32, 3]\n",
    "        return\n",
    "            - normalized x \n",
    "    \"\"\"\n",
    "    min_val = np.min(x) #typically will be zero\n",
    "    max_val = np.max(x) #typically will be 255\n",
    "    x = (x-min_val) / (max_val-min_val)\n",
    "    x[x<0.8]=0\n",
    "    return x\n",
    "def load_ben_color(path, sigmaX=10 ):\n",
    "    image = cv2.imread(path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n",
    "    image = equalize_light(image,3,(5,5))\n",
    "    #image=cv2.addWeighted ( image,4, cv2.GaussianBlur( image , (0,0) , sigmaX) ,-4 ,128)\n",
    "    image=normalize(image)  \n",
    "    return image\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "N = len(trainpaths)\n",
    "X_train = np.empty((N, IMG_SIZE, IMG_SIZE, 3), dtype=np.float64)\n",
    "\n",
    "for i, image_path in enumerate(tqdm(trainpaths)):\n",
    "    X_train[i, :, :, :] = load_ben_color(image_path,sigmaX=10)\n",
    "\n",
    "\n",
    "# In[14]:\n",
    "\n",
    "\n",
    "print(X_train.shape)\n",
    "\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "\n",
    "N_val = len(valpaths)\n",
    "X_val = np.empty((N_val, IMG_SIZE, IMG_SIZE, 3), dtype=np.float64)\n",
    "\n",
    "for i, image_path in enumerate(tqdm(valpaths)):\n",
    "    X_val[i, :, :, :] = load_ben_color(image_path,sigmaX=10)\n",
    "print(X_val.shape)\n",
    "\n",
    "\n",
    "# In[16]:\n",
    "\n",
    "\n",
    "N_test = len(testpaths)\n",
    "X_test = np.empty((N_test, IMG_SIZE, IMG_SIZE, 3), dtype=np.float64)\n",
    "\n",
    "for i, image_path in enumerate(tqdm(testpaths)):\n",
    "    X_test[i, :, :, :] = load_ben_color(image_path,sigmaX=10)\n",
    "print(X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Extracted Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "#with open(\"densenet121/extractedfeatures.pickle\", \"wb\") as f:\n",
    "#    pickle.dump((Train_Features,Train_Target,Val_Features,Val_Target,Test_Features,Test_Target,model,model_extracted), f)\n",
    "\n",
    "with open(thevars, \"rb\") as f:\n",
    "    Train_Features,Train_Target,Val_Features,Val_Target,Test_Features,Test_Target,model,model_extracted = pickle.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test=Test_Target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DenseNet169 output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(X_test, verbose=2)\n",
    "from sklearn.metrics import accuracy_score,f1_score,roc_curve,auc\n",
    "print(accuracy_score(Y_test,np.round(preds)))\n",
    "print(f1_score(Y_test,np.round(preds)))\n",
    "fpr, tpr, thresholds = roc_curve(Y_test, preds)\n",
    "print(\"AUC: \" + str(auc(fpr, tpr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict={\"val_loss\": [0.6326061271257319, 0.5416921238272877, 0.4986336695945869, 0.49928065698783275, 0.5944412969431635, 0.5316322253606582, 0.5619842170785039, 0.4867947189674034, 0.5710916241662483, 0.5526685400597625, 0.5170543382291571, 0.4294330312286393, 0.6615542507083234, 0.5495079302749897, 0.46326644586051924, 0.4509483315682007, 0.5450441941492639, 0.45706215440848114, 0.47182994102269915], \"val_acc\": [0.6610169491525424, 0.7457627118644068, 0.7711864406779662, 0.7372881355932204, 0.7372881355932204, 0.7457627118644068, 0.7372881355932204, 0.7796610169491526, 0.7203389830508474, 0.7796610169491526, 0.7457627118644068, 0.8305084745762712, 0.7203389830508474, 0.7372881355932204, 0.8050847457627118, 0.8050847457627118, 0.7711864406779662, 0.8220338983050848, 0.788135593220339], \"loss\": [0.6525186945410336, 0.5326157165976132, 0.4295450732287239, 0.37116500588024365, 0.3361167902104995, 0.28402104167377246, 0.3148369348750395, 0.2743631196022034, 0.2774100516824161, 0.2637738080585704, 0.237877601034501, 0.2554569693172679, 0.24783022123224596, 0.20921514048295864, 0.23769445966271793, 0.2305149124650394, 0.2021147998641519, 0.2062181026795331, 0.20249913944917566], \"acc\": [0.611764705882353, 0.7458823529411764, 0.8329411764705882, 0.8917647058823529, 0.9152941176470588, 0.9435294117647058, 0.9035294117647059, 0.9458823529411765, 0.9435294117647058, 0.9552941176470588, 0.971764705882353, 0.9435294117647058, 0.9482352941176471, 0.9811764705882353, 0.9647058823529412, 0.9647058823529412, 0.9835294117647059, 0.9741176470588235, 0.9788235294117648]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#historyfilename ='model/MondayDesneNet169/model_weights.h5'\n",
    "#history_dict = json.load(open(historyfilename, 'r'))\n",
    "\n",
    "history_df = pd.DataFrame(history_dict)\n",
    "history_df.to_csv('history.csv')\n",
    "history_df[['loss', 'val_loss']].plot()\n",
    "history_df[['acc', 'val_acc']].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "def measure_per(y_val,preds,num_classes):\n",
    "    print(accuracy_score(y_val,preds))\n",
    "    print(cohen_kappa_score(y_val,preds))\n",
    "\n",
    "    \n",
    "    df_cm = pd.DataFrame(cm, range(num_classes),\n",
    "                              range(num_classes))\n",
    "    print(df_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes=2\n",
    "# Creating a function to report confusion metrics\n",
    "def confusion_metrics (truvalues,predictions):\n",
    "# save confusion matrix and slice into four pieces\n",
    "    conf_matrix = confusion_matrix(truvalues, predictions)\n",
    "    df_cm = pd.DataFrame(conf_matrix, range(num_classes),\n",
    "                              range(num_classes))\n",
    "    print(df_cm)\n",
    "    TP = conf_matrix[1][1]\n",
    "    TN = conf_matrix[0][0]\n",
    "    FP = conf_matrix[0][1]\n",
    "    FN = conf_matrix[1][0]\n",
    "    print('True Positives:', TP)\n",
    "    print('True Negatives:', TN)\n",
    "    print('False Positives:', FP)\n",
    "    print('False Negatives:', FN)\n",
    "    \n",
    "    # calculate accuracy\n",
    "    conf_accuracy = (float (TP+TN) / float(TP + TN + FP + FN))\n",
    "    \n",
    "    # calculate mis-classification\n",
    "    conf_misclassification = 1- conf_accuracy\n",
    "    \n",
    "    # calculate the sensitivity\n",
    "    conf_sensitivity = (TP / float(TP + FN))\n",
    "    # calculate the specificity\n",
    "    conf_specificity = (TN / float(TN + FP))\n",
    "    \n",
    "    # calculate precision\n",
    "    conf_precision = (TN / float(TN + FP))\n",
    "    # calculate f_1 score\n",
    "    conf_f1 = 2 * ((conf_precision * conf_sensitivity) / (conf_precision + conf_sensitivity))\n",
    "    print('-'*50)\n",
    "    print(f'Accuracy: {round(conf_accuracy,2)}') \n",
    "    print(f'Mis-Classification: {round(conf_misclassification,2)}') \n",
    "    print(f'Sensitivity: {round(conf_sensitivity,2)}') \n",
    "    print(f'Specificity: {round(conf_specificity,2)}') \n",
    "    print(f'Precision: {round(conf_precision,2)}')\n",
    "    print(f'f_1 Score: {round(conf_f1,2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "min_max_scaler = preprocessing.MinMaxScaler(feature_range=(-1,1))\n",
    "X_train_minmax = min_max_scaler.fit_transform(Train_Features)\n",
    "X_val_minmax = min_max_scaler.transform(Val_Features)\n",
    "X_Test_minmax = min_max_scaler.transform(Test_Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(X_train_minmax[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.8763848396501457\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier,XGBRegressor\n",
    "from xgboost import plot_importance,plot_tree\n",
    "eval_set = [(Val_Features, Val_Target)]\n",
    "\n",
    "objective='binary:logistic'\n",
    "eval_metric=['error']\n",
    "\n",
    "\n",
    "xgb = XGBClassifier(objective=objective,learning_rate=0.12,\n",
    "                   n_estimators=200,subsample=1,\n",
    "                   max_depth=11,n_jobs=3)#binary:logistic\n",
    "\n",
    "xgb.fit(Train_Features, Train_Target,eval_metric=eval_metric, eval_set=eval_set,verbose=False,early_stopping_rounds=50)\n",
    "\n",
    "preds_xgb = xgb.predict(Test_Features)\n",
    "\n",
    "from sklearn.metrics import accuracy_score,f1_score,roc_curve,auc\n",
    "fpr, tpr, thresholds = roc_curve(Test_Target, xgb.predict_proba(Test_Features)[:,1])\n",
    "print(\"AUC: \" + str(auc(fpr, tpr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0   1\n",
      "0  86  19\n",
      "1  21  77\n",
      "True Positives: 77\n",
      "True Negatives: 86\n",
      "False Positives: 19\n",
      "False Negatives: 21\n",
      "--------------------------------------------------\n",
      "Accuracy: 0.8\n",
      "Mis-Classification: 0.2\n",
      "Sensitivity: 0.79\n",
      "Specificity: 0.82\n",
      "Precision: 0.82\n",
      "f_1 Score: 0.8\n"
     ]
    }
   ],
   "source": [
    "confusion_metrics(Test_Target,preds_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(\"bestXGBoostmodel.pickle\", \"wb\") as f:\n",
    "#    pickle.dump((xgb), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"bestRFmodel/bestRFmodel.pickle\", \"rb\") as f:\n",
    "    rfa = pickle.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.8961613216715256\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(max_depth=10, n_estimators=400)\n",
    "rf.fit(Train_Features, Train_Target)\n",
    "preds_rf = rf.predict(Test_Features)\n",
    "fpr, tpr, thresholds = roc_curve(Y_test, rf.predict_proba(Test_Features)[:,1])\n",
    "print(\"AUC: \" + str(auc(fpr, tpr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0   1\n",
      "0  85  20\n",
      "1  19  79\n",
      "True Positives: 79\n",
      "True Negatives: 85\n",
      "False Positives: 20\n",
      "False Negatives: 19\n",
      "--------------------------------------------------\n",
      "Accuracy: 0.81\n",
      "Mis-Classification: 0.19\n",
      "Sensitivity: 0.81\n",
      "Specificity: 0.81\n",
      "Precision: 0.81\n",
      "f_1 Score: 0.81\n"
     ]
    }
   ],
   "source": [
    "confusion_metrics(Y_test,preds_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_rf = rf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth=None, min_samples_split=2,random_state=0)\n",
    "bdt = BaggingClassifier(n_estimators=200,base_estimator=clf).fit(Train_Features, Train_Target)\n",
    "#Predict the response for test dataset\n",
    "preds_bdt = bdt.predict(Test_Features)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(Y_test, bdt.predict_proba(Test_Features)[:,1])\n",
    "print(\"AUC: \" + str(auc(fpr, tpr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_metrics(Y_test,preds_bdt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"bestbdtmodel.pickle\", \"wb\") as f:\n",
    "    pickle.dump((bdt), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "clf = AdaBoostClassifier(n_estimators=200)\n",
    "clf.fit(Train_Features,Train_Target)\n",
    "#Predict the response for test dataset\n",
    "preds_adaboost = clf.predict(Test_Features)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(Y_test, clf.predict_proba(Test_Features)[:,1])\n",
    "print(\"AUC: \" + str(auc(fpr, tpr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_metrics(Y_test,preds_adaboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"bestadaboostmodel.pickle\", \"wb\") as f:\n",
    "    pickle.dump((clf), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GBDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm\n",
    "train_data = lightgbm.Dataset(Train_Features, label=np.squeeze(Train_Target))\n",
    "val_data = lightgbm.Dataset(Val_Features, label=np.squeeze(Val_Target))\n",
    "\n",
    "parameters = {\n",
    "    'objective': 'cross_entropy',\n",
    "    'boosting': 'gbdt',#'gbdt' 'dart'\n",
    "    'num_leaves': 21,\n",
    "    'feature_fraction': 0.99,\n",
    "    'bagging_fraction': 0.99,\n",
    "    'bagging_freq': 30,  'learning_rate': 0.1,\n",
    "    'verbose': 2,\n",
    "}\n",
    "\n",
    "gbm = lightgbm.train(parameters,\n",
    "                       train_data,\n",
    "                       valid_sets=val_data,\n",
    "                       num_boost_round=1000,\n",
    "                       early_stopping_rounds=100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_gbdt=gbm.predict(Test_Features)\n",
    "fpr, tpr, thresholds = roc_curve(Y_test, preds_gbdt)\n",
    "print(\"AUC: \" + str(auc(fpr, tpr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_metrics(Y_test,np.round(preds_gbdt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"bestlightgbmmodel.pickle\", \"wb\") as f:\n",
    "    pickle.dump((gbm), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm\n",
    "train_data = lightgbm.Dataset(Train_Features, label=np.squeeze(Train_Target))\n",
    "val_data = lightgbm.Dataset(Val_Features, label=np.squeeze(Val_Target))\n",
    "\n",
    "parameters = {\n",
    "    'objective': 'binary',\n",
    "    'boosting': 'dart',#'gbdt' 'dart'\n",
    "    'num_leaves': 31,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.9,\n",
    "    'bagging_freq': 30,\n",
    "    'verbose': 2,\n",
    "}\n",
    "\n",
    "dart = lightgbm.train(parameters,\n",
    "                       train_data,\n",
    "                       valid_sets=val_data,\n",
    "                       num_boost_round=1000,\n",
    "                       early_stopping_rounds=100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_dart=dart.predict(Test_Features)\n",
    "fpr, tpr, thresholds = roc_curve(Y_test, preds_dart)\n",
    "print(\"AUC: \" + str(auc(fpr, tpr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_metrics(Y_test,np.round(preds_dart))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"bestdartmmodel.pickle\", \"wb\") as f:\n",
    "    pickle.dump((dart), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import KMeans\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create kmeans object\n",
    "kmeans = KMeans(n_clusters=2)\n",
    "# fit kmeans object to data\n",
    "kmeans.fit(Train_Features)\n",
    "# print location of clusters learned by kmeans object\n",
    "print(kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_km = kmeans.fit_predict(Train_Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = kmeans.fit_predict(Test_Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_metrics(Y_test,s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "di={'actual':Y_test,'preds_xgboost':preds_xgb,'preds_rf':preds_rf,\n",
    "   'preds_bdt':preds_bdt,'preds_adaboost':preds_adaboost,'preds_gbdt':np.round(preds_gbdt),'preds_dart':np.round(preds_dart)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(di)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC,LinearSVC, NuSVC\n",
    "svm = SVC(kernel='rbf',C=1)\n",
    "svm.fit(Train_Features, Train_Target)#sometimes labels instead\n",
    "print('fitting done !!!')\n",
    "\n",
    "\n",
    "preds=svm.predict(Test_Features)\n",
    "confusion_metrics(Y_test,preds)\n",
    "fpr, tpr, thresholds = roc_curve(Y_test, preds)\n",
    "print(\"AUC: \" + str(auc(fpr, tpr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=300)\n",
    "x_train_pca = pca.fit_transform(Train_Features)\n",
    "x_test_pca=pca.transform(Test_Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.svm import NuSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier,OneVsOneClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import neighbors\n",
    "\n",
    "kfold = model_selection.KFold(n_splits=5, random_state=5)\n",
    "# create the sub models\n",
    "estimators = []\n",
    "model1 = SVC(kernel='rbf')\n",
    "estimators.append(('model1', model1))\n",
    "\n",
    "model2 = neighbors.KNeighborsClassifier(13, weights='distance')\n",
    "estimators.append(('model2', model2))\n",
    "\n",
    "\n",
    "model3 = GaussianNB()\n",
    "estimators.append(('model3', model3))\n",
    "# create the ensemble model\n",
    "ensemble = VotingClassifier(estimators)\n",
    "\n",
    "ensemble.fit(Train_Features, Train_Target)\n",
    "#results = model_selection.cross_val_score(ensemble, x_train, y_train, cv=kfold)\n",
    "#print(results.mean())\n",
    "preds=ensemble.predict(Test_Features)\n",
    "confusion_metrics(Y_test,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e=ensemble.predict_proba(Test_Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(Y_test, e)\n",
    "print(\"AUC: \" + str(auc(fpr, tpr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "bc = BaggingClassifier(base_estimator=SVC(kernel='rbf'),n_estimators=3, random_state=0).fit(Train_Features, Train_Target)\n",
    "preds=bc.predict(Test_Features)\n",
    "confusion_metrics(Y_test,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e=bc.predict_proba(Test_Features)[:,1]\n",
    "fpr, tpr, thresholds = roc_curve(Y_test, e)\n",
    "print(\"AUC: \" + str(auc(fpr, tpr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basec=SVC(probability=True, kernel='rbf')\n",
    "# Create adaboost classifer object\n",
    "abc = AdaBoostClassifier(n_estimators=5,\n",
    "                         learning_rate=0.1,base_estimator=basec).fit(Train_Features, Train_Target)\n",
    "preds=abc.predict(Test_Features)\n",
    "confusion_metrics(Y_test,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
